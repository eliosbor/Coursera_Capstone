{"cells": [{"metadata": {"collapsed": true}, "cell_type": "code", "source": "#In this section I will use location data from the Phoenix area regarding where most restaurants are located, where restaurants receive the highest amount of business, and where restaurants make the most money.", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib notebook", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_unprocessed = pd.read_pickle('./yelp_dataset_processed/yelp_df_all_final.pkl')\ndf_reviews = pd.read_json('./yelp_old/yelp_training_set/yelp_training_set_review.json',lines=True)", "execution_count": 2, "outputs": [{"output_type": "error", "ename": "FileNotFoundError", "evalue": "[Errno 2] No such file or directory: './yelp_dataset_processed/yelp_df_all_final.pkl'", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)", "\u001b[0;32m<ipython-input-2-e14bc8766450>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_unprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./yelp_dataset_processed/yelp_df_all_final.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./yelp_old/yelp_training_set/yelp_training_set_review.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# 1) try standard library Pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './yelp_dataset_processed/yelp_df_all_final.pkl'"]}]}, {"metadata": {}, "cell_type": "code", "source": "df_unprocessed.head()", "execution_count": 3, "outputs": [{"output_type": "error", "ename": "NameError", "evalue": "name 'df_unprocessed' is not defined", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "\u001b[0;32m<ipython-input-3-3fb881cd98e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_unprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", "\u001b[0;31mNameError\u001b[0m: name 'df_unprocessed' is not defined"]}]}, {"metadata": {}, "cell_type": "code", "source": "df_unprocessed.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(df_unprocessed[df_unprocessed['is_closed']==1])/len(df_unprocessed)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['stars'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['stars'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'])\n\n\nplt.legend()\nplt.title('Yelp Stars Histogram',fontsize=14)\nplt.xlabel('Yelp Star Rating',fontsize=14)\nplt.ylabel('Total Number of Restaurants',fontsize=16)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['stars'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['stars'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'],normed=True)\n\n\nplt.legend()\nplt.title('Yelp Stars Histogram')\nplt.xlabel('Yelp Star Rating')\nplt.ylabel('Integral Value')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "open_length = len(df_unprocessed[df_unprocessed['is_closed']==0].dropna()['stars'])\nclosed_length = len(df_unprocessed[df_unprocessed['is_closed']==1].dropna()['stars'])\nweights = [np.ones(open_length)/open_length,np.ones(closed_length)/closed_length]\nplt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['stars'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['stars'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'],weights=weights)\n\n\nplt.legend()\nplt.title('Yelp Stars Histogram Percentages',fontsize=16)\nplt.xlabel('Yelp Star Rating',fontsize=14)\nplt.ylabel('Percentages per Category (%)',fontsize=14)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['review_count'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['review_count'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'],log=True)\n\n\nplt.legend()\nplt.title('Number of Reviews Histogram')\nplt.xlabel('Total Number of Reviews')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['price'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['price'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'])\n\n\nplt.legend()\nplt.title('Price')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml = df_unprocessed[['review_count','stars','price','oldest_review','std_of_stars','reviews_per_week',\\\n                        'median_of_stars','reactions_per_week','stars_linear_coef','restaurant_density',\\\n                        'restaurant_similar_density','zreview_count_all','zstar_all','zprice_all','zreview_per_week_all',\\\n                        'is_claimed','is_chain','is_closed']]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(df_ml)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml_clean = df_ml.dropna(axis = 0)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(df_ml_clean)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(df_ml_clean[df_ml_clean['is_closed']==1])/len(df_ml_clean)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml_features = df_ml_clean.drop('is_closed',axis = 1)\ndf_ml_target = df_ml_clean['is_closed']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml_features", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = LogisticRegression(random_state=0)\nclf_A = GaussianNB()\nclf_B = DecisionTreeClassifier(random_state=1)\nclf_C = GradientBoostingClassifier(random_state=1)\nclf_D = RandomForestClassifier(random_state=1)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_test, y_train, y_test = train_test_split(df_ml_features, df_ml_target, test_size = 0.2, random_state = 10,\\\n                                                    stratify = df_ml_target)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf.fit(X_train,list(y_train.values))\ny_pred = clf.predict(X_test)\nprint('Accuracy: ',clf.score(X_test,list(y_test.values)))\nprint('Precision: ',precision_score(list(y_test.values),y_pred))\nprint('Recall: ',recall_score(list(y_test.values),y_pred))\nprint('F1 Score: ',f1_score(list(y_test.values),y_pred))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_pred))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf_A.fit(X_train,list(y_train.values))\ny_pred_A = clf_A.predict(X_test)\nprint('Accuracy: ',clf_A.score(X_test,list(y_test.values)))\nprint('Precision: ',precision_score(list(y_test.values),y_pred_A))\nprint('Recall: ',recall_score(list(y_test.values),y_pred_A))\nprint('F1 Score: ',f1_score(list(y_test.values),y_pred_A))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_pred_A))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf_B.fit(X_train,list(y_train.values))\ny_pred_B = clf_B.predict(X_test)\nprint('Accuracy: ',clf_B.score(X_test,list(y_test.values)))\nprint('Precision: ',precision_score(list(y_test.values),y_pred_B))\nprint('Recall: ',recall_score(list(y_test.values),y_pred_B))\nprint('F1 Score: ',f1_score(list(y_test.values),y_pred_B))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_pred_B))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf_C.fit(X_train,list(y_train.values))\ny_pred_C = clf_C.predict(X_test)\nprint('Accuracy: ',clf_C.score(X_test,list(y_test.values)))\nprint('Precision: ',precision_score(list(y_test.values),y_pred_C))\nprint('Recall: ',recall_score(list(y_test.values),y_pred_C))\nprint('F1 Score: ',f1_score(list(y_test.values),y_pred_C))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_pred_C))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf_D.fit(X_train,list(y_train.values))\ny_pred_D = clf_D.predict(X_test)\nprint('Accuracy: ',clf_D.score(X_test,list(y_test.values)))\nprint('Precision: ',precision_score(list(y_test.values),y_pred_D))\nprint('Recall: ',recall_score(list(y_test.values),y_pred_D))\nprint('F1 Score: ',f1_score(list(y_test.values),y_pred_D))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test.values), y_pred_D))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y_score = clf.decision_function(X_test)\ny_score_A = clf_A.predict_proba(X_test)[:,1]\ny_score_B = clf_B.predict_proba(X_test)[:,1]\ny_score_C = clf_C.decision_function(X_test)\ny_score_D = clf_D.predict_proba(X_test)[:,1]\nfpr, tpr, _ = roc_curve(y_test.values,y_score)\nfpr_A, tpr_A, _ = roc_curve(y_test.values,y_score_A)\nfpr_B, tpr_B, _ = roc_curve(y_test.values,y_score_B)\nfpr_C, tpr_C, _ = roc_curve(y_test.values,y_score_C)\nfpr_D, tpr_D, _ = roc_curve(y_test.values,y_score_D)\nplt.plot([0,1],[0,1],'k--')\n\nplt.plot(fpr,tpr,label='Logistic Regression')\nplt.plot(fpr_A,tpr_A,label='Gaussian NB')\nplt.plot(fpr_B,tpr_B,label='Decision Tree')\nplt.plot(fpr_C,tpr_C,label='Gradient Boosting')\nplt.plot(fpr_D,tpr_D,label='Random Forest')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fpr, tpr, _ = roc_curve(y_test.values,y_score)\nfpr_C, tpr_C, _ = roc_curve(y_test.values,y_score_C)\n\nplt.plot([0,1],[0,1],'k--')\n\nplt.plot(fpr,tpr,label='Logistic Regression')\n\nplt.plot(fpr_C,tpr_C,label='Gradient Boosting')\n\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curves')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "importances = clf_D.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_D.estimators_],axis=0)\nindices = np.argsort(importances)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_test.shape[1]):\n    print(\"%d. feature: %s (%f)\" % (f + 1, df_ml_features.iloc[:,indices[f]].name, importances[indices[f]]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml_features.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "labels = ['Review Count', 'Star Rating', 'Price', 'Age', 'Std of Star Rating',\\\n          'Reviews per Week', 'Median of Star Rating', 'Reaction to Reviews per Week',\\\n          'Linear Coef of Stars', 'Restaurant Density', 'Similar Restaurant Density',\\\n          'Relative Review Count', 'Relative Star Rating', 'Relative Price', 'Relative Reviews per Week',\\\n          'Business Claimed', 'Chain']\nlen(labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.figure(figsize=(10,7))\nplt.title(\"Feature importance\")\nplt.barh(range(X_test.shape[1]), importances[indices],\n       color=\"#c41200\", xerr=std[indices], align=\"center\")\n#plt.yticks(range(X_test.shape[1]), df_ml_features.iloc[:,indices].columns)\nplt.yticks(range(X_test.shape[1]), [labels[i] for i in indices])\nplt.ylim([-1, X_test.shape[1]])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Gradient Boosting importances\nimportances2 = clf_C.feature_importances_\n#std2 = np.std([tree.feature_importances_ for tree in clf_C.estimators_],axis=0)\nindices2 = np.argsort(importances2)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_test.shape[1]):\n    print(\"%d. feature: %s (%f)\" % (f + 1, df_ml_features.iloc[:,indices2[f]].name, importances2[indices2[f]]))\n    #print(\"%d. feature: %s (%f)\" % (f + 1, df_ml_features.columns[indices[f]], importances[indices[f]]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Gradient boosting\n% matplotlib inline\nplt.figure(figsize=(10,7))\nplt.title(\"Feature importance\")\nplt.barh(range(X_test.shape[1]), importances2[indices2],\n       color=\"#c41200\", align=\"center\")\n#plt.yticks(range(X_test.shape[1]), df_ml_features.iloc[:,indices].columns)\nplt.yticks(range(X_test.shape[1]), [labels[i] for i in indices2])\nplt.ylim([-1, X_test.shape[1]])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['zprice_all'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['zprice_all'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'])\n\n\nplt.legend()\nplt.title('Relative Price Histogram')\nplt.xlabel('Price Relative to Neighboring Restaurants')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['stars_linear_coef'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['stars_linear_coef'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'],bins=5,log=True)\n\n\nplt.legend()\nplt.title('Linear Coefficient of Stars Histogram')\nplt.xlabel('Review Star Linear Coefficient with Time')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.hist([df_unprocessed[df_unprocessed['is_closed']==0].dropna()['oldest_review'].values,\\\n          df_unprocessed[df_unprocessed['is_closed']==1].dropna()['oldest_review'].values],\\\n        label=['Open','Closed'],color=['k','#c41200'])\n\n\nplt.legend()\nplt.title('Age Histogram')\nplt.xlabel('Days since Oldest Review')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.preprocessing import StandardScaler", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "scaler = StandardScaler()\nscaler.fit(df_ml_features)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml_features_scaled = scaler.transform(df_ml_features)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(df_ml_features_scaled, \\\n                                                    df_ml_target, test_size = 0.2, random_state = 10,\\\n                                                    stratify = df_ml_target)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf.fit(X_train_scaled,list(y_train_scaled.values))\ny_pred_scaled = clf.predict(X_test_scaled)\nprint('Accuracy: ',clf.score(X_test_scaled,list(y_test_scaled.values)))\nprint('Precision: ',precision_score(list(y_test_scaled.values),y_pred_scaled))\nprint('Recall: ',recall_score(list(y_test_scaled.values),y_pred_scaled))\nprint('F1 Score: ',f1_score(list(y_test_scaled.values),y_pred_scaled))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test_scaled.values), y_pred_scaled))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf.fit(X_train_scaled,list(y_train_scaled.values))\ny_pred_scaled = clf.predict(X_test_scaled)\nprint('Accuracy: ',clf.score(X_test_scaled,list(y_test_scaled.values)))\nprint('Precision: ',precision_score(list(y_test_scaled.values),y_pred_scaled,pos_label=False))\nprint('Recall: ',recall_score(list(y_test_scaled.values),y_pred_scaled,pos_label=False))\nprint('F1 Score: ',f1_score(list(y_test_scaled.values),y_pred_scaled,pos_label=False))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test_scaled.values), y_pred_scaled))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf_C.fit(X_train_scaled,list(y_train_scaled.values))\ny_pred_scaled_C = clf_C.predict(X_test_scaled)\nprint('Accuracy: ',clf_C.score(X_test_scaled,list(y_test_scaled.values)))\nprint('Precision: ',precision_score(list(y_test_scaled.values),y_pred_scaled_C,pos_label=False))\nprint('Recall: ',recall_score(list(y_test_scaled.values),y_pred_scaled_C,pos_label=False))\nprint('F1 Score: ',f1_score(list(y_test_scaled.values),y_pred_scaled_C,pos_label=False))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test_scaled.values), y_pred_scaled_C))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Reds):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nplot_confusion_matrix(confusion_matrix(list(y_test_scaled.values), y_pred_scaled_C),\\\n                          classes=['Open','Closed'],title='Confusion matrix')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "param_grid = {'penalty' : ['l1','l2'],'C': [0.01,0.1,1.,10.], 'intercept_scaling': [0.0005,0.001,0.005,0.01,0.1,1.,10.]}\nscorer = make_scorer(precision_score,pos_label=False)\ngscv = GridSearchCV(clf,param_grid,scoring=scorer)\ngscv.fit(X_train_scaled,list(y_train_scaled.values))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "gscv.best_params_", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "gscv.best_score_", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "clf_optimized = gscv.best_estimator_", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y_pred_scaled = clf_optimized.predict(X_test_scaled)\nprint('Accuracy: ',clf_optimized.score(X_test_scaled,list(y_test_scaled.values)))\nprint('Precision: ',precision_score(list(y_test_scaled.values),y_pred_scaled))\nprint('Recall: ',recall_score(list(y_test_scaled.values),y_pred_scaled))\nprint('F1 Score: ',f1_score(list(y_test_scaled.values),y_pred_scaled))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test_scaled.values), y_pred_scaled))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y_pred_scaled = clf_optimized.predict(X_test_scaled)\nprint('Accuracy: ',clf_optimized.score(X_test_scaled,list(y_test_scaled.values)))\nprint('Precision: ',precision_score(list(y_test_scaled.values),y_pred_scaled,pos_label=False))\nprint('Recall: ',recall_score(list(y_test_scaled.values),y_pred_scaled,pos_label=False))\nprint('F1 Score: ',f1_score(list(y_test_scaled.values),y_pred_scaled,pos_label=False))\nprint('Confusion Matrix: \\n',confusion_matrix(list(y_test_scaled.values), y_pred_scaled,labels=[True,False]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Reds):\n    \"\"\"\n    This function prints and plots the confusion matrix.\nNormalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ny_pred_scaled = clf_optimized.predict(X_test_scaled)\nplot_confusion_matrix(confusion_matrix(list(y_test_scaled.values), y_pred_scaled),\\\n                          classes=['Open','Closed'],title='Confusion matrix')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "y_score_scaled = clf_optimized.decision_function(X_test_scaled)\nfpr, tpr, _ = roc_curve(list(y_test_scaled.values),y_score_scaled)\n\nplt.plot([0,1],[0,1],'k--')\n\nplt.plot(fpr,tpr,'#c41200',label='Logistic Regression')\n\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_ml_features.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "np.std(X_test_scaled,axis=0)*clf_optimized.coef_[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "abs(np.std(X_test_scaled,axis=0)*clf_optimized.coef_)\n# Importances with signs\nimportances_scaled_sign = np.std(X_test_scaled,axis=0)*clf_optimized.coef_[0]\n# Absolute value of importances\nimportances_scaled = abs(np.std(X_test_scaled,axis=0)*clf_optimized.coef_[0])\n\nindices_scaled = np.argsort(importances_scaled)[::-1]\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train_scaled.shape[1]):\n    print(\"%d. feature: %s (%f)\" % (f + 1, df_ml_features.iloc[:,indices_scaled[f]].name,\\\n                                    importances_scaled_sign[indices_scaled[f]]))", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}